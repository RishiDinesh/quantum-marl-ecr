{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9558215a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "from typing import Any, Dict, List, Tuple, Union\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import Adam, RMSprop\n",
    "\n",
    "from maro.rl.model import DiscreteACBasedNet, FullyConnected, VNet, DiscreteQNet\n",
    "from maro.rl.policy import DiscretePolicyGradient\n",
    "from maro.rl.rl_component.rl_component_bundle import RLComponentBundle\n",
    "from maro.rl.rollout import AbsEnvSampler, CacheElement, ExpElement\n",
    "from maro.rl.training import TrainingManager\n",
    "from maro.simulator import Env\n",
    "from maro.simulator.scenarios.cim.common import Action, ActionType, DecisionEvent\n",
    "\n",
    "from maro.rl.exploration import epsilon_greedy\n",
    "from maro.rl.policy import ValueBasedPolicy\n",
    "from maro.rl.training.algorithms import DQNParams, DQNTrainer\n",
    "\n",
    "\n",
    "from maro.rl.training.algorithms import PPOParams, PPOTrainer\n",
    "\n",
    "from maro.rl.model import DiscreteACBasedNet, FullyConnected, MultiQNet\n",
    "from maro.rl.policy import DiscretePolicyGradient\n",
    "from maro.rl.training.algorithms import DiscreteMADDPGParams, DiscreteMADDPGTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0600aa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env and shaping config\n",
    "reward_shaping_conf = {\n",
    "    \"time_window\": 99,\n",
    "    \"fulfillment_factor\": 1.0,\n",
    "    \"shortage_factor\": 1.0,\n",
    "    \"time_decay\": 0.97,\n",
    "}\n",
    "state_shaping_conf = {\n",
    "    \"look_back\": 7,\n",
    "    \"max_ports_downstream\": 2,\n",
    "}\n",
    "port_attributes = [\"empty\", \"full\", \"on_shipper\", \"on_consignee\", \"booking\", \"shortage\", \"fulfillment\"]\n",
    "vessel_attributes = [\"empty\", \"full\", \"remaining_space\"]\n",
    "action_shaping_conf = {\n",
    "    \"action_space\": [(i - 10) / 10 for i in range(21)],\n",
    "    \"finite_vessel_space\": True,\n",
    "    \"has_early_discharge\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4f7aa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIMEnvSampler(AbsEnvSampler):\n",
    "    def _get_global_and_agent_state_impl(\n",
    "        self,\n",
    "        event: DecisionEvent,\n",
    "        tick: int = None,\n",
    "    ) -> Tuple[Union[None, np.ndarray, List[object]], Dict[Any, Union[np.ndarray, List[object]]]]:\n",
    "        tick = self._env.tick\n",
    "        vessel_snapshots, port_snapshots = self._env.snapshot_list[\"vessels\"], self._env.snapshot_list[\"ports\"]\n",
    "        port_idx, vessel_idx = event.port_idx, event.vessel_idx\n",
    "        ticks = [max(0, tick - rt) for rt in range(state_shaping_conf[\"look_back\"] - 1)]\n",
    "        future_port_list = vessel_snapshots[tick:vessel_idx:\"future_stop_list\"].astype(\"int\")\n",
    "        state = np.concatenate(\n",
    "            [\n",
    "                port_snapshots[ticks : [port_idx] + list(future_port_list) : port_attributes],\n",
    "                vessel_snapshots[tick:vessel_idx:vessel_attributes],\n",
    "            ],\n",
    "        )\n",
    "        return state, {port_idx: state}\n",
    "\n",
    "    def _translate_to_env_action(\n",
    "        self,\n",
    "        action_dict: Dict[Any, Union[np.ndarray, List[object]]],\n",
    "        event: DecisionEvent,\n",
    "    ) -> Dict[Any, object]:\n",
    "        action_space = action_shaping_conf[\"action_space\"]\n",
    "        finite_vsl_space = action_shaping_conf[\"finite_vessel_space\"]\n",
    "        has_early_discharge = action_shaping_conf[\"has_early_discharge\"]\n",
    "\n",
    "        port_idx, model_action = list(action_dict.items()).pop()\n",
    "\n",
    "        vsl_idx, action_scope = event.vessel_idx, event.action_scope\n",
    "        vsl_snapshots = self._env.snapshot_list[\"vessels\"]\n",
    "        vsl_space = vsl_snapshots[self._env.tick : vsl_idx : vessel_attributes][2] if finite_vsl_space else float(\"inf\")\n",
    "\n",
    "        percent = abs(action_space[model_action[0]])\n",
    "        zero_action_idx = len(action_space) / 2  # index corresponding to value zero.\n",
    "        if model_action < zero_action_idx:\n",
    "            action_type = ActionType.LOAD\n",
    "            actual_action = min(round(percent * action_scope.load), vsl_space)\n",
    "        elif model_action > zero_action_idx:\n",
    "            action_type = ActionType.DISCHARGE\n",
    "            early_discharge = (\n",
    "                vsl_snapshots[self._env.tick : vsl_idx : \"early_discharge\"][0] if has_early_discharge else 0\n",
    "            )\n",
    "            plan_action = percent * (action_scope.discharge + early_discharge) - early_discharge\n",
    "            actual_action = round(plan_action) if plan_action > 0 else round(percent * action_scope.discharge)\n",
    "        else:\n",
    "            actual_action, action_type = 0, None\n",
    "\n",
    "        return {port_idx: Action(vsl_idx, int(port_idx), actual_action, action_type)}\n",
    "\n",
    "    def _get_reward(self, env_action_dict: Dict[Any, object], event: DecisionEvent, tick: int) -> Dict[Any, float]:\n",
    "        start_tick = tick + 1\n",
    "        ticks = list(range(start_tick, start_tick + reward_shaping_conf[\"time_window\"]))\n",
    "\n",
    "        # Get the ports that took actions at the given tick\n",
    "        ports = [int(port) for port in list(env_action_dict.keys())]\n",
    "        port_snapshots = self._env.snapshot_list[\"ports\"]\n",
    "        future_fulfillment = port_snapshots[ticks:ports:\"fulfillment\"].reshape(len(ticks), -1)\n",
    "        future_shortage = port_snapshots[ticks:ports:\"shortage\"].reshape(len(ticks), -1)\n",
    "\n",
    "        decay_list = [reward_shaping_conf[\"time_decay\"] ** i for i in range(reward_shaping_conf[\"time_window\"])]\n",
    "        rewards = np.float32(\n",
    "            reward_shaping_conf[\"fulfillment_factor\"] * np.dot(future_fulfillment.T, decay_list)\n",
    "            - reward_shaping_conf[\"shortage_factor\"] * np.dot(future_shortage.T, decay_list),\n",
    "        )\n",
    "        return {agent_id: reward for agent_id, reward in zip(ports, rewards)}\n",
    "\n",
    "    def _post_step(self, cache_element: CacheElement) -> None:\n",
    "        self._info[\"env_metric\"] = self._env.metrics\n",
    "\n",
    "    def _post_eval_step(self, cache_element: CacheElement) -> None:\n",
    "        self._post_step(cache_element)\n",
    "\n",
    "    def post_collect(self, info_list: list, ep: int) -> None:\n",
    "        # print the env metric from each rollout worker\n",
    "        for info in info_list:\n",
    "            print(f\"env summary (episode {ep}): {info['env_metric']}\")\n",
    "\n",
    "        # average env metric\n",
    "        metric_keys, num_envs = info_list[0][\"env_metric\"].keys(), len(info_list)\n",
    "        avg_metric = {key: sum(info[\"env_metric\"][key] for info in info_list) / num_envs for key in metric_keys}\n",
    "        print(f\"average env summary (episode {ep}): {avg_metric}\")\n",
    "\n",
    "        self.metrics.update(avg_metric)\n",
    "        self.metrics = {k: v for k, v in self.metrics.items() if not k.startswith(\"val/\")}\n",
    "\n",
    "    def post_evaluate(self, info_list: list, ep: int) -> None:\n",
    "        # print the env metric from each rollout worker\n",
    "        for info in info_list:\n",
    "            print(f\"env summary (episode {ep}): {info['env_metric']}\")\n",
    "\n",
    "        # average env metric\n",
    "        metric_keys, num_envs = info_list[0][\"env_metric\"].keys(), len(info_list)\n",
    "        avg_metric = {key: sum(info[\"env_metric\"][key] for info in info_list) / num_envs for key in metric_keys}\n",
    "        print(f\"average env summary (episode {ep}): {avg_metric}\")\n",
    "\n",
    "        self.metrics.update({\"val/\" + k: v for k, v in avg_metric.items()})\n",
    "\n",
    "    def monitor_metrics(self) -> float:\n",
    "        return -self.metrics[\"val/container_shortage\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac173777",
   "metadata": {},
   "source": [
    "# Actor - Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b219900e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Adam, RMSprop\n",
    "\n",
    "from maro.rl.model import DiscreteACBasedNet, FullyConnected, VNet\n",
    "from maro.rl.policy import DiscretePolicyGradient\n",
    "from maro.rl.training.algorithms import ActorCriticParams, ActorCriticTrainer\n",
    "\n",
    "state_dim = (\n",
    "    (state_shaping_conf[\"look_back\"] + 1) * (state_shaping_conf[\"max_ports_downstream\"] + 1) * len(port_attributes)\n",
    "    + len(vessel_attributes)\n",
    ")\n",
    "action_num = len(action_shaping_conf[\"action_space\"])\n",
    "\n",
    "actor_net_conf = {\n",
    "    \"hidden_dims\": [256, 128, 64],\n",
    "    \"activation\": torch.nn.Tanh,\n",
    "    \"output_activation\": torch.nn.Tanh,\n",
    "    \"softmax\": True,\n",
    "    \"batch_norm\": False,\n",
    "    \"head\": True,\n",
    "}\n",
    "critic_net_conf = {\n",
    "    \"hidden_dims\": [256, 128, 64],\n",
    "    \"output_dim\": 1,\n",
    "    \"activation\": torch.nn.LeakyReLU,\n",
    "    \"output_activation\": torch.nn.LeakyReLU,\n",
    "    \"softmax\": False,\n",
    "    \"batch_norm\": True,\n",
    "    \"head\": True,\n",
    "}\n",
    "actor_learning_rate = 0.001\n",
    "critic_learning_rate = 0.001\n",
    "\n",
    "\n",
    "class MyActorNet(DiscreteACBasedNet):\n",
    "    def __init__(self, state_dim: int, action_num: int) -> None:\n",
    "        super(MyActorNet, self).__init__(state_dim=state_dim, action_num=action_num)\n",
    "        self._actor = FullyConnected(input_dim=state_dim, output_dim=action_num, **actor_net_conf)\n",
    "        self._optim = Adam(self._actor.parameters(), lr=actor_learning_rate)\n",
    "\n",
    "    def _get_action_probs_impl(self, states: torch.Tensor) -> torch.Tensor:\n",
    "        return self._actor(states)\n",
    "\n",
    "\n",
    "class MyCriticNet(VNet):\n",
    "    def __init__(self, state_dim: int) -> None:\n",
    "        super(MyCriticNet, self).__init__(state_dim=state_dim)\n",
    "        self._critic = FullyConnected(input_dim=state_dim, **critic_net_conf)\n",
    "        self._optim = RMSprop(self._critic.parameters(), lr=critic_learning_rate)\n",
    "\n",
    "    def _get_v_values(self, states: torch.Tensor) -> torch.Tensor:\n",
    "        return self._critic(states).squeeze(-1)\n",
    "\n",
    "\n",
    "def get_ac_policy(state_dim: int, action_num: int, name: str) -> DiscretePolicyGradient:\n",
    "    return DiscretePolicyGradient(name=name, policy_net=MyActorNet(state_dim, action_num))\n",
    "\n",
    "\n",
    "def get_ac(state_dim: int, name: str) -> ActorCriticTrainer:\n",
    "    return ActorCriticTrainer(\n",
    "        name=name,\n",
    "        reward_discount=0.0,\n",
    "        params=ActorCriticParams(\n",
    "            get_v_critic_net_func=lambda: MyCriticNet(state_dim),\n",
    "            grad_iters=10,\n",
    "            critic_loss_cls=torch.nn.SmoothL1Loss,\n",
    "            min_logp=None,\n",
    "            lam=0.0,\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0dbf5e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_env = Env(scenario=\"cim\", topology=\"toy.4p_ssdd_l0.0\", durations=500,options={\"enable-dump-snapshot\": \"./ac_learn\"})\n",
    "test_env = Env(scenario=\"cim\", topology=\"toy.4p_ssdd_l0.1\", durations=500,options={\"enable-dump-snapshot\": \"./ac_test\"})\n",
    "num_agents = len(learn_env.agent_idx_list)\n",
    "agent2policy = {agent: f\"ac_{agent}.policy\" for agent in learn_env.agent_idx_list}\n",
    "policies = [get_ac_policy(state_dim, action_num, f\"ac_{i}.policy\") for i in range(num_agents)]\n",
    "trainers = [get_ac(state_dim, f\"ac_{i}\") for i in range(num_agents)]\n",
    "\n",
    "rl_component_bundle = RLComponentBundle(\n",
    "    env_sampler=CIMEnvSampler(\n",
    "        learn_env=learn_env,\n",
    "        test_env=test_env,\n",
    "        policies=policies,\n",
    "        agent2policy=agent2policy,\n",
    "        reward_eval_delay=reward_shaping_conf[\"time_window\"],\n",
    "    ),\n",
    "    agent2policy=agent2policy,\n",
    "    policies=policies,\n",
    "    trainers=trainers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fecc407b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting result:\n",
      "env summary (episode 1): {'order_requirements': 1000000, 'container_shortage': 669920, 'operation_number': 1747069}\n",
      "average env summary (episode 1): {'order_requirements': 1000000.0, 'container_shortage': 669920.0, 'operation_number': 1747069.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 2): {'order_requirements': 1000000, 'container_shortage': 692457, 'operation_number': 1578829}\n",
      "average env summary (episode 2): {'order_requirements': 1000000.0, 'container_shortage': 692457.0, 'operation_number': 1578829.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 3): {'order_requirements': 1000000, 'container_shortage': 518699, 'operation_number': 1969736}\n",
      "average env summary (episode 3): {'order_requirements': 1000000.0, 'container_shortage': 518699.0, 'operation_number': 1969736.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 4): {'order_requirements': 1000000, 'container_shortage': 584074, 'operation_number': 2005531}\n",
      "average env summary (episode 4): {'order_requirements': 1000000.0, 'container_shortage': 584074.0, 'operation_number': 2005531.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 5): {'order_requirements': 1000000, 'container_shortage': 524280, 'operation_number': 1896748}\n",
      "average env summary (episode 5): {'order_requirements': 1000000.0, 'container_shortage': 524280.0, 'operation_number': 1896748.0}\n",
      "\n",
      "Evaluation result:\n",
      "env summary (episode 5): {'order_requirements': 1000000, 'container_shortage': 803613, 'operation_number': 287033}\n",
      "average env summary (episode 5): {'order_requirements': 1000000.0, 'container_shortage': 803613.0, 'operation_number': 287033.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 6): {'order_requirements': 1000000, 'container_shortage': 516765, 'operation_number': 1990787}\n",
      "average env summary (episode 6): {'order_requirements': 1000000.0, 'container_shortage': 516765.0, 'operation_number': 1990787.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 7): {'order_requirements': 1000000, 'container_shortage': 461780, 'operation_number': 1779496}\n",
      "average env summary (episode 7): {'order_requirements': 1000000.0, 'container_shortage': 461780.0, 'operation_number': 1779496.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 8): {'order_requirements': 1000000, 'container_shortage': 556608, 'operation_number': 2018041}\n",
      "average env summary (episode 8): {'order_requirements': 1000000.0, 'container_shortage': 556608.0, 'operation_number': 2018041.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 9): {'order_requirements': 1000000, 'container_shortage': 537244, 'operation_number': 1974551}\n",
      "average env summary (episode 9): {'order_requirements': 1000000.0, 'container_shortage': 537244.0, 'operation_number': 1974551.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 10): {'order_requirements': 1000000, 'container_shortage': 525085, 'operation_number': 1702337}\n",
      "average env summary (episode 10): {'order_requirements': 1000000.0, 'container_shortage': 525085.0, 'operation_number': 1702337.0}\n",
      "\n",
      "Evaluation result:\n",
      "env summary (episode 10): {'order_requirements': 1000000, 'container_shortage': 785420, 'operation_number': 315395}\n",
      "average env summary (episode 10): {'order_requirements': 1000000.0, 'container_shortage': 785420.0, 'operation_number': 315395.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 11): {'order_requirements': 1000000, 'container_shortage': 510942, 'operation_number': 1822506}\n",
      "average env summary (episode 11): {'order_requirements': 1000000.0, 'container_shortage': 510942.0, 'operation_number': 1822506.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 12): {'order_requirements': 1000000, 'container_shortage': 580576, 'operation_number': 1575616}\n",
      "average env summary (episode 12): {'order_requirements': 1000000.0, 'container_shortage': 580576.0, 'operation_number': 1575616.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 13): {'order_requirements': 1000000, 'container_shortage': 444644, 'operation_number': 1772720}\n",
      "average env summary (episode 13): {'order_requirements': 1000000.0, 'container_shortage': 444644.0, 'operation_number': 1772720.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 14): {'order_requirements': 1000000, 'container_shortage': 506433, 'operation_number': 1546717}\n",
      "average env summary (episode 14): {'order_requirements': 1000000.0, 'container_shortage': 506433.0, 'operation_number': 1546717.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 15): {'order_requirements': 1000000, 'container_shortage': 495374, 'operation_number': 1686731}\n",
      "average env summary (episode 15): {'order_requirements': 1000000.0, 'container_shortage': 495374.0, 'operation_number': 1686731.0}\n",
      "\n",
      "Evaluation result:\n",
      "env summary (episode 15): {'order_requirements': 1000000, 'container_shortage': 788930, 'operation_number': 307870}\n",
      "average env summary (episode 15): {'order_requirements': 1000000.0, 'container_shortage': 788930.0, 'operation_number': 307870.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 16): {'order_requirements': 1000000, 'container_shortage': 430390, 'operation_number': 1837072}\n",
      "average env summary (episode 16): {'order_requirements': 1000000.0, 'container_shortage': 430390.0, 'operation_number': 1837072.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 17): {'order_requirements': 1000000, 'container_shortage': 306273, 'operation_number': 1797933}\n",
      "average env summary (episode 17): {'order_requirements': 1000000.0, 'container_shortage': 306273.0, 'operation_number': 1797933.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 18): {'order_requirements': 1000000, 'container_shortage': 453587, 'operation_number': 1519787}\n",
      "average env summary (episode 18): {'order_requirements': 1000000.0, 'container_shortage': 453587.0, 'operation_number': 1519787.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 19): {'order_requirements': 1000000, 'container_shortage': 443250, 'operation_number': 1930826}\n",
      "average env summary (episode 19): {'order_requirements': 1000000.0, 'container_shortage': 443250.0, 'operation_number': 1930826.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 20): {'order_requirements': 1000000, 'container_shortage': 501375, 'operation_number': 1280591}\n",
      "average env summary (episode 20): {'order_requirements': 1000000.0, 'container_shortage': 501375.0, 'operation_number': 1280591.0}\n",
      "\n",
      "Evaluation result:\n",
      "env summary (episode 20): {'order_requirements': 1000000, 'container_shortage': 801066, 'operation_number': 284530}\n",
      "average env summary (episode 20): {'order_requirements': 1000000.0, 'container_shortage': 801066.0, 'operation_number': 284530.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 21): {'order_requirements': 1000000, 'container_shortage': 469831, 'operation_number': 1649262}\n",
      "average env summary (episode 21): {'order_requirements': 1000000.0, 'container_shortage': 469831.0, 'operation_number': 1649262.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 22): {'order_requirements': 1000000, 'container_shortage': 447889, 'operation_number': 1782394}\n",
      "average env summary (episode 22): {'order_requirements': 1000000.0, 'container_shortage': 447889.0, 'operation_number': 1782394.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 23): {'order_requirements': 1000000, 'container_shortage': 509703, 'operation_number': 1445426}\n",
      "average env summary (episode 23): {'order_requirements': 1000000.0, 'container_shortage': 509703.0, 'operation_number': 1445426.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 24): {'order_requirements': 1000000, 'container_shortage': 488584, 'operation_number': 1443079}\n",
      "average env summary (episode 24): {'order_requirements': 1000000.0, 'container_shortage': 488584.0, 'operation_number': 1443079.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 25): {'order_requirements': 1000000, 'container_shortage': 378096, 'operation_number': 1661003}\n",
      "average env summary (episode 25): {'order_requirements': 1000000.0, 'container_shortage': 378096.0, 'operation_number': 1661003.0}\n",
      "\n",
      "Evaluation result:\n",
      "env summary (episode 25): {'order_requirements': 1000000, 'container_shortage': 850717, 'operation_number': 212364}\n",
      "average env summary (episode 25): {'order_requirements': 1000000.0, 'container_shortage': 850717.0, 'operation_number': 212364.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 26): {'order_requirements': 1000000, 'container_shortage': 406619, 'operation_number': 1524496}\n",
      "average env summary (episode 26): {'order_requirements': 1000000.0, 'container_shortage': 406619.0, 'operation_number': 1524496.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 27): {'order_requirements': 1000000, 'container_shortage': 372192, 'operation_number': 1690690}\n",
      "average env summary (episode 27): {'order_requirements': 1000000.0, 'container_shortage': 372192.0, 'operation_number': 1690690.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 28): {'order_requirements': 1000000, 'container_shortage': 347556, 'operation_number': 1708129}\n",
      "average env summary (episode 28): {'order_requirements': 1000000.0, 'container_shortage': 347556.0, 'operation_number': 1708129.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 29): {'order_requirements': 1000000, 'container_shortage': 396767, 'operation_number': 1624189}\n",
      "average env summary (episode 29): {'order_requirements': 1000000.0, 'container_shortage': 396767.0, 'operation_number': 1624189.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 30): {'order_requirements': 1000000, 'container_shortage': 342501, 'operation_number': 1689924}\n",
      "average env summary (episode 30): {'order_requirements': 1000000.0, 'container_shortage': 342501.0, 'operation_number': 1689924.0}\n",
      "\n",
      "Evaluation result:\n",
      "env summary (episode 30): {'order_requirements': 1000000, 'container_shortage': 950000, 'operation_number': 0}\n",
      "average env summary (episode 30): {'order_requirements': 1000000.0, 'container_shortage': 950000.0, 'operation_number': 0.0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env_sampler = rl_component_bundle.env_sampler\n",
    "\n",
    "num_episodes = 30\n",
    "eval_schedule = [5,10,15,20,25,30]\n",
    "eval_point_index = 0\n",
    "\n",
    "training_manager = TrainingManager(rl_component_bundle=rl_component_bundle)\n",
    "\n",
    "# main loop\n",
    "for ep in range(1, num_episodes + 1):\n",
    "    result = env_sampler.sample()\n",
    "    experiences: List[List[ExpElement]] = result[\"experiences\"]\n",
    "    info_list: List[dict] = result[\"info\"]\n",
    "        \n",
    "    print(\"Collecting result:\")\n",
    "    env_sampler.post_collect(info_list, ep)\n",
    "    print()\n",
    "\n",
    "    training_manager.record_experiences(experiences)\n",
    "    training_manager.train_step()\n",
    "\n",
    "    if ep == eval_schedule[eval_point_index]:\n",
    "        eval_point_index += 1\n",
    "        result = env_sampler.eval()\n",
    "        \n",
    "        print(\"Evaluation result:\")\n",
    "        env_sampler.post_evaluate(result[\"info\"], ep)\n",
    "        print()\n",
    "\n",
    "training_manager.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cea4f08",
   "metadata": {},
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "94841542",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_net_conf = {\n",
    "    \"hidden_dims\": [256, 128, 64, 32],\n",
    "    \"activation\": torch.nn.LeakyReLU,\n",
    "    \"output_activation\": torch.nn.LeakyReLU,\n",
    "    \"softmax\": False,\n",
    "    \"batch_norm\": True,\n",
    "    \"skip_connection\": False,\n",
    "    \"head\": True,\n",
    "    \"dropout_p\": 0.0,\n",
    "}\n",
    "learning_rate = 0.05\n",
    "\n",
    "\n",
    "class MyQNet(DiscreteQNet):\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim: int,\n",
    "        action_num: int,\n",
    "        dueling_param: Optional[Tuple[dict, dict]] = None,\n",
    "    ) -> None:\n",
    "        super(MyQNet, self).__init__(state_dim=state_dim, action_num=action_num)\n",
    "\n",
    "        self._use_dueling = dueling_param is not None\n",
    "        self._fc = FullyConnected(input_dim=state_dim, output_dim=1 if self._use_dueling else action_num, **q_net_conf)\n",
    "        if self._use_dueling:\n",
    "            q_kwargs, v_kwargs = dueling_param\n",
    "            self._q = FullyConnected(input_dim=self._fc.output_dim, output_dim=action_num, **q_kwargs)\n",
    "            self._v = FullyConnected(input_dim=self._fc.output_dim, output_dim=1, **v_kwargs)\n",
    "\n",
    "        self._optim = RMSprop(self.parameters(), lr=learning_rate)\n",
    "\n",
    "    def _get_q_values_for_all_actions(self, states: torch.Tensor) -> torch.Tensor:\n",
    "        logits = self._fc(states)\n",
    "        if self._use_dueling:\n",
    "            q = self._q(logits)\n",
    "            v = self._v(logits)\n",
    "            logits = q - q.mean(dim=1, keepdim=True) + v\n",
    "        return logits\n",
    "\n",
    "\n",
    "def get_dqn_policy(state_dim: int, action_num: int, name: str) -> ValueBasedPolicy:\n",
    "    q_kwargs = {\n",
    "        \"hidden_dims\": [128],\n",
    "        \"activation\": torch.nn.LeakyReLU,\n",
    "        \"output_activation\": torch.nn.LeakyReLU,\n",
    "        \"softmax\": False,\n",
    "        \"batch_norm\": True,\n",
    "        \"skip_connection\": False,\n",
    "        \"head\": True,\n",
    "        \"dropout_p\": 0.0,\n",
    "    }\n",
    "    v_kwargs = {\n",
    "        \"hidden_dims\": [128],\n",
    "        \"activation\": torch.nn.LeakyReLU,\n",
    "        \"output_activation\": None,\n",
    "        \"softmax\": False,\n",
    "        \"batch_norm\": True,\n",
    "        \"skip_connection\": False,\n",
    "        \"head\": True,\n",
    "        \"dropout_p\": 0.0,\n",
    "    }\n",
    "\n",
    "    return ValueBasedPolicy(\n",
    "        name=name,\n",
    "        q_net=MyQNet(\n",
    "            state_dim,\n",
    "            action_num,\n",
    "            dueling_param=(q_kwargs, v_kwargs),\n",
    "        ),\n",
    "        exploration_strategy=(epsilon_greedy, {\"epsilon\": 0.4}),\n",
    "        warmup=100,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_dqn(name: str) -> DQNTrainer:\n",
    "    return DQNTrainer(\n",
    "        name=name,\n",
    "        reward_discount=0.0,\n",
    "        replay_memory_capacity=10000,\n",
    "        batch_size=32,\n",
    "        params=DQNParams(\n",
    "            update_target_every=5,\n",
    "            num_epochs=10,\n",
    "            soft_update_coef=0.1,\n",
    "            double=False\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9e39e079",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_env = Env(scenario=\"cim\", topology=\"toy.4p_ssdd_l0.0\", durations=500,options={\"enable-dump-snapshot\": \"./dqn_learn\"})\n",
    "test_env = Env(scenario=\"cim\", topology=\"toy.4p_ssdd_l0.1\", durations=500,options={\"enable-dump-snapshot\": \"./dqn_test\"})\n",
    "num_agents = len(learn_env.agent_idx_list)\n",
    "agent2policy = {agent: f\"dqn_{agent}.policy\" for agent in learn_env.agent_idx_list}\n",
    "policies = [get_dqn_policy(state_dim, action_num, f\"dqn_{i}.policy\") for i in range(num_agents)]\n",
    "trainers = [get_dqn(f\"dqn_{i}\") for i in range(num_agents)]\n",
    "\n",
    "rl_component_bundle = RLComponentBundle(\n",
    "    env_sampler=CIMEnvSampler(\n",
    "        learn_env=learn_env,\n",
    "        test_env=test_env,\n",
    "        policies=policies,\n",
    "        agent2policy=agent2policy,\n",
    "        reward_eval_delay=reward_shaping_conf[\"time_window\"],\n",
    "    ),\n",
    "    agent2policy=agent2policy,\n",
    "    policies=policies,\n",
    "    trainers=trainers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d03d4c0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting result:\n",
      "env summary (episode 1): {'order_requirements': 1000000, 'container_shortage': 677164, 'operation_number': 1597412}\n",
      "average env summary (episode 1): {'order_requirements': 1000000.0, 'container_shortage': 677164.0, 'operation_number': 1597412.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 2): {'order_requirements': 1000000, 'container_shortage': 945986, 'operation_number': 136577}\n",
      "average env summary (episode 2): {'order_requirements': 1000000.0, 'container_shortage': 945986.0, 'operation_number': 136577.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 3): {'order_requirements': 1000000, 'container_shortage': 976645, 'operation_number': 106695}\n",
      "average env summary (episode 3): {'order_requirements': 1000000.0, 'container_shortage': 976645.0, 'operation_number': 106695.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 4): {'order_requirements': 1000000, 'container_shortage': 967329, 'operation_number': 98635}\n",
      "average env summary (episode 4): {'order_requirements': 1000000.0, 'container_shortage': 967329.0, 'operation_number': 98635.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 5): {'order_requirements': 1000000, 'container_shortage': 949280, 'operation_number': 210375}\n",
      "average env summary (episode 5): {'order_requirements': 1000000.0, 'container_shortage': 949280.0, 'operation_number': 210375.0}\n",
      "\n",
      "Evaluation result:\n",
      "env summary (episode 5): {'order_requirements': 1000000, 'container_shortage': 954832, 'operation_number': 9664}\n",
      "average env summary (episode 5): {'order_requirements': 1000000.0, 'container_shortage': 954832.0, 'operation_number': 9664.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 6): {'order_requirements': 1000000, 'container_shortage': 962852, 'operation_number': 25704}\n",
      "average env summary (episode 6): {'order_requirements': 1000000.0, 'container_shortage': 962852.0, 'operation_number': 25704.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 7): {'order_requirements': 1000000, 'container_shortage': 962852, 'operation_number': 60704}\n",
      "average env summary (episode 7): {'order_requirements': 1000000.0, 'container_shortage': 962852.0, 'operation_number': 60704.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 8): {'order_requirements': 1000000, 'container_shortage': 962852, 'operation_number': 60704}\n",
      "average env summary (episode 8): {'order_requirements': 1000000.0, 'container_shortage': 962852.0, 'operation_number': 60704.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 9): {'order_requirements': 1000000, 'container_shortage': 336223, 'operation_number': 1566117}\n",
      "average env summary (episode 9): {'order_requirements': 1000000.0, 'container_shortage': 336223.0, 'operation_number': 1566117.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 10): {'order_requirements': 1000000, 'container_shortage': 822956, 'operation_number': 734991}\n",
      "average env summary (episode 10): {'order_requirements': 1000000.0, 'container_shortage': 822956.0, 'operation_number': 734991.0}\n",
      "\n",
      "Evaluation result:\n",
      "env summary (episode 10): {'order_requirements': 1000000, 'container_shortage': 832921, 'operation_number': 274456}\n",
      "average env summary (episode 10): {'order_requirements': 1000000.0, 'container_shortage': 832921.0, 'operation_number': 274456.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 11): {'order_requirements': 1000000, 'container_shortage': 729922, 'operation_number': 1391259}\n",
      "average env summary (episode 11): {'order_requirements': 1000000.0, 'container_shortage': 729922.0, 'operation_number': 1391259.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 12): {'order_requirements': 1000000, 'container_shortage': 211623, 'operation_number': 1708054}\n",
      "average env summary (episode 12): {'order_requirements': 1000000.0, 'container_shortage': 211623.0, 'operation_number': 1708054.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 13): {'order_requirements': 1000000, 'container_shortage': 756799, 'operation_number': 597502}\n",
      "average env summary (episode 13): {'order_requirements': 1000000.0, 'container_shortage': 756799.0, 'operation_number': 597502.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 14): {'order_requirements': 1000000, 'container_shortage': 371308, 'operation_number': 2199616}\n",
      "average env summary (episode 14): {'order_requirements': 1000000.0, 'container_shortage': 371308.0, 'operation_number': 2199616.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 15): {'order_requirements': 1000000, 'container_shortage': 954733, 'operation_number': 23664}\n",
      "average env summary (episode 15): {'order_requirements': 1000000.0, 'container_shortage': 954733.0, 'operation_number': 23664.0}\n",
      "\n",
      "Evaluation result:\n",
      "env summary (episode 15): {'order_requirements': 1000000, 'container_shortage': 777457, 'operation_number': 368143}\n",
      "average env summary (episode 15): {'order_requirements': 1000000.0, 'container_shortage': 777457.0, 'operation_number': 368143.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 16): {'order_requirements': 1000000, 'container_shortage': 90699, 'operation_number': 1862066}\n",
      "average env summary (episode 16): {'order_requirements': 1000000.0, 'container_shortage': 90699.0, 'operation_number': 1862066.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 17): {'order_requirements': 1000000, 'container_shortage': 803006, 'operation_number': 400918}\n",
      "average env summary (episode 17): {'order_requirements': 1000000.0, 'container_shortage': 803006.0, 'operation_number': 400918.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 18): {'order_requirements': 1000000, 'container_shortage': 811727, 'operation_number': 386744}\n",
      "average env summary (episode 18): {'order_requirements': 1000000.0, 'container_shortage': 811727.0, 'operation_number': 386744.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 19): {'order_requirements': 1000000, 'container_shortage': 859837, 'operation_number': 248776}\n",
      "average env summary (episode 19): {'order_requirements': 1000000.0, 'container_shortage': 859837.0, 'operation_number': 248776.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 20): {'order_requirements': 1000000, 'container_shortage': 777105, 'operation_number': 480967}\n",
      "average env summary (episode 20): {'order_requirements': 1000000.0, 'container_shortage': 777105.0, 'operation_number': 480967.0}\n",
      "\n",
      "Evaluation result:\n",
      "env summary (episode 20): {'order_requirements': 1000000, 'container_shortage': 833757, 'operation_number': 308060}\n",
      "average env summary (episode 20): {'order_requirements': 1000000.0, 'container_shortage': 833757.0, 'operation_number': 308060.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 21): {'order_requirements': 1000000, 'container_shortage': 360488, 'operation_number': 1686411}\n",
      "average env summary (episode 21): {'order_requirements': 1000000.0, 'container_shortage': 360488.0, 'operation_number': 1686411.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 22): {'order_requirements': 1000000, 'container_shortage': 803609, 'operation_number': 505450}\n",
      "average env summary (episode 22): {'order_requirements': 1000000.0, 'container_shortage': 803609.0, 'operation_number': 505450.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 23): {'order_requirements': 1000000, 'container_shortage': 879996, 'operation_number': 296121}\n",
      "average env summary (episode 23): {'order_requirements': 1000000.0, 'container_shortage': 879996.0, 'operation_number': 296121.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 24): {'order_requirements': 1000000, 'container_shortage': 944588, 'operation_number': 100376}\n",
      "average env summary (episode 24): {'order_requirements': 1000000.0, 'container_shortage': 944588.0, 'operation_number': 100376.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 25): {'order_requirements': 1000000, 'container_shortage': 926304, 'operation_number': 115707}\n",
      "average env summary (episode 25): {'order_requirements': 1000000.0, 'container_shortage': 926304.0, 'operation_number': 115707.0}\n",
      "\n",
      "Evaluation result:\n",
      "env summary (episode 25): {'order_requirements': 1000000, 'container_shortage': 942768, 'operation_number': 69515}\n",
      "average env summary (episode 25): {'order_requirements': 1000000.0, 'container_shortage': 942768.0, 'operation_number': 69515.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 26): {'order_requirements': 1000000, 'container_shortage': 949035, 'operation_number': 75420}\n",
      "average env summary (episode 26): {'order_requirements': 1000000.0, 'container_shortage': 949035.0, 'operation_number': 75420.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 27): {'order_requirements': 1000000, 'container_shortage': 953541, 'operation_number': 77725}\n",
      "average env summary (episode 27): {'order_requirements': 1000000.0, 'container_shortage': 953541.0, 'operation_number': 77725.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 28): {'order_requirements': 1000000, 'container_shortage': 953541, 'operation_number': 77725}\n",
      "average env summary (episode 28): {'order_requirements': 1000000.0, 'container_shortage': 953541.0, 'operation_number': 77725.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 29): {'order_requirements': 1000000, 'container_shortage': 951920, 'operation_number': 78376}\n",
      "average env summary (episode 29): {'order_requirements': 1000000.0, 'container_shortage': 951920.0, 'operation_number': 78376.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 30): {'order_requirements': 1000000, 'container_shortage': 951920, 'operation_number': 78376}\n",
      "average env summary (episode 30): {'order_requirements': 1000000.0, 'container_shortage': 951920.0, 'operation_number': 78376.0}\n",
      "\n",
      "Evaluation result:\n",
      "env summary (episode 30): {'order_requirements': 1000000, 'container_shortage': 931076, 'operation_number': 88600}\n",
      "average env summary (episode 30): {'order_requirements': 1000000.0, 'container_shortage': 931076.0, 'operation_number': 88600.0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env_sampler = rl_component_bundle.env_sampler\n",
    "\n",
    "num_episodes = 30\n",
    "eval_schedule = [5,10,15,20,25,30]\n",
    "eval_point_index = 0\n",
    "\n",
    "training_manager = TrainingManager(rl_component_bundle=rl_component_bundle)\n",
    "for ep in range(1, num_episodes + 1):\n",
    "    result = env_sampler.sample()\n",
    "    experiences: List[List[ExpElement]] = result[\"experiences\"]\n",
    "    info_list: List[dict] = result[\"info\"]\n",
    "        \n",
    "    print(\"Collecting result:\")\n",
    "    env_sampler.post_collect(info_list, ep)\n",
    "    print()\n",
    "\n",
    "    training_manager.record_experiences(experiences)\n",
    "    training_manager.train_step()\n",
    "\n",
    "    if ep == eval_schedule[eval_point_index]:\n",
    "        eval_point_index += 1\n",
    "        result = env_sampler.eval()\n",
    "        \n",
    "        print(\"Evaluation result:\")\n",
    "        env_sampler.post_evaluate(result[\"info\"], ep)\n",
    "        print()\n",
    "\n",
    "training_manager.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa278e9e",
   "metadata": {},
   "source": [
    "# MADDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ef7b9422",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_net_conf = {\n",
    "    \"hidden_dims\": [256, 128, 64],\n",
    "    \"activation\": torch.nn.Tanh,\n",
    "    \"output_activation\": torch.nn.Tanh,\n",
    "    \"softmax\": True,\n",
    "    \"batch_norm\": False,\n",
    "    \"head\": True,\n",
    "}\n",
    "critic_net_conf = {\n",
    "    \"hidden_dims\": [256, 128, 64],\n",
    "    \"output_dim\": 1,\n",
    "    \"activation\": torch.nn.LeakyReLU,\n",
    "    \"output_activation\": torch.nn.LeakyReLU,\n",
    "    \"softmax\": False,\n",
    "    \"batch_norm\": True,\n",
    "    \"head\": True,\n",
    "}\n",
    "actor_learning_rate = 0.001\n",
    "critic_learning_rate = 0.001\n",
    "\n",
    "class MyActorNet(DiscreteACBasedNet):\n",
    "    def __init__(self, state_dim: int, action_num: int) -> None:\n",
    "        super(MyActorNet, self).__init__(state_dim=state_dim, action_num=action_num)\n",
    "        self._actor = FullyConnected(input_dim=state_dim, output_dim=action_num, **actor_net_conf)\n",
    "        self._optim = Adam(self._actor.parameters(), lr=actor_learning_rate)\n",
    "\n",
    "    def _get_action_probs_impl(self, states: torch.Tensor) -> torch.Tensor:\n",
    "        return self._actor(states)\n",
    "\n",
    "\n",
    "class MyMultiCriticNet(MultiQNet):\n",
    "    def __init__(self, state_dim: int, action_dims: List[int]) -> None:\n",
    "        super(MyMultiCriticNet, self).__init__(state_dim=state_dim, action_dims=action_dims)\n",
    "        self._critic = FullyConnected(input_dim=state_dim + sum(action_dims), **critic_net_conf)\n",
    "        self._optim = RMSprop(self._critic.parameters(), critic_learning_rate)\n",
    "\n",
    "    def _get_q_values(self, states: torch.Tensor, actions: List[torch.Tensor]) -> torch.Tensor:\n",
    "        return self._critic(torch.cat([states] + actions, dim=1)).squeeze(-1)\n",
    "\n",
    "\n",
    "def get_multi_critic_net(state_dim: int, action_dims: List[int]) -> MyMultiCriticNet:\n",
    "    return MyMultiCriticNet(state_dim, action_dims)\n",
    "\n",
    "\n",
    "def get_maddpg_policy(state_dim: int, action_num: int, name: str) -> DiscretePolicyGradient:\n",
    "    return DiscretePolicyGradient(name=name, policy_net=MyActorNet(state_dim, action_num))\n",
    "\n",
    "\n",
    "def get_maddpg(state_dim: int, action_dims: List[int], name: str) -> DiscreteMADDPGTrainer:\n",
    "    return DiscreteMADDPGTrainer(\n",
    "        name=name,\n",
    "        reward_discount=0.0,\n",
    "        params=DiscreteMADDPGParams(\n",
    "            num_epoch=10,\n",
    "            get_q_critic_net_func=partial(get_multi_critic_net, state_dim, action_dims),\n",
    "            shared_critic=False,\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8cc21ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_env = Env(scenario=\"cim\", topology=\"toy.4p_ssdd_l0.0\", durations=500,options={\"enable-dump-snapshot\": \"./maddpg_learn\"})\n",
    "test_env = Env(scenario=\"cim\", topology=\"toy.4p_ssdd_l0.1\", durations=500,options={\"enable-dump-snapshot\": \"./maddpg_test\"})\n",
    "\n",
    "num_agents = len(learn_env.agent_idx_list)\n",
    "agent2policy = {agent: f\"maddpg_{agent}.policy\" for agent in learn_env.agent_idx_list}\n",
    "policies = [get_maddpg_policy(state_dim, action_num, f\"maddpg_{i}.policy\") for i in range(num_agents)]\n",
    "trainers = [get_maddpg(state_dim, [1], f\"maddpg_{i}\") for i in range(num_agents)]\n",
    "\n",
    "# Build RLComponentBundle\n",
    "rl_component_bundle = RLComponentBundle(\n",
    "    env_sampler=CIMEnvSampler(\n",
    "        learn_env=learn_env,\n",
    "        test_env=test_env,\n",
    "        policies=policies,\n",
    "        agent2policy=agent2policy,\n",
    "        reward_eval_delay=reward_shaping_conf[\"time_window\"],\n",
    "    ),\n",
    "    agent2policy=agent2policy,\n",
    "    policies=policies,\n",
    "    trainers=trainers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c1405a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting result:\n",
      "env summary (episode 1): {'order_requirements': 1000000, 'container_shortage': 678834, 'operation_number': 1804825}\n",
      "average env summary (episode 1): {'order_requirements': 1000000.0, 'container_shortage': 678834.0, 'operation_number': 1804825.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 2): {'order_requirements': 1000000, 'container_shortage': 538220, 'operation_number': 1745516}\n",
      "average env summary (episode 2): {'order_requirements': 1000000.0, 'container_shortage': 538220.0, 'operation_number': 1745516.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 3): {'order_requirements': 1000000, 'container_shortage': 471884, 'operation_number': 1915979}\n",
      "average env summary (episode 3): {'order_requirements': 1000000.0, 'container_shortage': 471884.0, 'operation_number': 1915979.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 4): {'order_requirements': 1000000, 'container_shortage': 385978, 'operation_number': 1995230}\n",
      "average env summary (episode 4): {'order_requirements': 1000000.0, 'container_shortage': 385978.0, 'operation_number': 1995230.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 5): {'order_requirements': 1000000, 'container_shortage': 318185, 'operation_number': 1647935}\n",
      "average env summary (episode 5): {'order_requirements': 1000000.0, 'container_shortage': 318185.0, 'operation_number': 1647935.0}\n",
      "\n",
      "Evaluation result:\n",
      "env summary (episode 5): {'order_requirements': 1000000, 'container_shortage': 858741, 'operation_number': 200924}\n",
      "average env summary (episode 5): {'order_requirements': 1000000.0, 'container_shortage': 858741.0, 'operation_number': 200924.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 6): {'order_requirements': 1000000, 'container_shortage': 274234, 'operation_number': 1834262}\n",
      "average env summary (episode 6): {'order_requirements': 1000000.0, 'container_shortage': 274234.0, 'operation_number': 1834262.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 7): {'order_requirements': 1000000, 'container_shortage': 302901, 'operation_number': 1456314}\n",
      "average env summary (episode 7): {'order_requirements': 1000000.0, 'container_shortage': 302901.0, 'operation_number': 1456314.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 8): {'order_requirements': 1000000, 'container_shortage': 282125, 'operation_number': 1609059}\n",
      "average env summary (episode 8): {'order_requirements': 1000000.0, 'container_shortage': 282125.0, 'operation_number': 1609059.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 9): {'order_requirements': 1000000, 'container_shortage': 340470, 'operation_number': 1495052}\n",
      "average env summary (episode 9): {'order_requirements': 1000000.0, 'container_shortage': 340470.0, 'operation_number': 1495052.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 10): {'order_requirements': 1000000, 'container_shortage': 393983, 'operation_number': 1278512}\n",
      "average env summary (episode 10): {'order_requirements': 1000000.0, 'container_shortage': 393983.0, 'operation_number': 1278512.0}\n",
      "\n",
      "Evaluation result:\n",
      "env summary (episode 10): {'order_requirements': 1000000, 'container_shortage': 870343, 'operation_number': 175146}\n",
      "average env summary (episode 10): {'order_requirements': 1000000.0, 'container_shortage': 870343.0, 'operation_number': 175146.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 11): {'order_requirements': 1000000, 'container_shortage': 366258, 'operation_number': 1393865}\n",
      "average env summary (episode 11): {'order_requirements': 1000000.0, 'container_shortage': 366258.0, 'operation_number': 1393865.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 12): {'order_requirements': 1000000, 'container_shortage': 380690, 'operation_number': 1360789}\n",
      "average env summary (episode 12): {'order_requirements': 1000000.0, 'container_shortage': 380690.0, 'operation_number': 1360789.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 13): {'order_requirements': 1000000, 'container_shortage': 493480, 'operation_number': 1091419}\n",
      "average env summary (episode 13): {'order_requirements': 1000000.0, 'container_shortage': 493480.0, 'operation_number': 1091419.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 14): {'order_requirements': 1000000, 'container_shortage': 498316, 'operation_number': 919704}\n",
      "average env summary (episode 14): {'order_requirements': 1000000.0, 'container_shortage': 498316.0, 'operation_number': 919704.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 15): {'order_requirements': 1000000, 'container_shortage': 358079, 'operation_number': 1276524}\n",
      "average env summary (episode 15): {'order_requirements': 1000000.0, 'container_shortage': 358079.0, 'operation_number': 1276524.0}\n",
      "\n",
      "Evaluation result:\n",
      "env summary (episode 15): {'order_requirements': 1000000, 'container_shortage': 764498, 'operation_number': 355813}\n",
      "average env summary (episode 15): {'order_requirements': 1000000.0, 'container_shortage': 764498.0, 'operation_number': 355813.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 16): {'order_requirements': 1000000, 'container_shortage': 425213, 'operation_number': 1103860}\n",
      "average env summary (episode 16): {'order_requirements': 1000000.0, 'container_shortage': 425213.0, 'operation_number': 1103860.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 17): {'order_requirements': 1000000, 'container_shortage': 384904, 'operation_number': 1192103}\n",
      "average env summary (episode 17): {'order_requirements': 1000000.0, 'container_shortage': 384904.0, 'operation_number': 1192103.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 18): {'order_requirements': 1000000, 'container_shortage': 522585, 'operation_number': 866372}\n",
      "average env summary (episode 18): {'order_requirements': 1000000.0, 'container_shortage': 522585.0, 'operation_number': 866372.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 19): {'order_requirements': 1000000, 'container_shortage': 322683, 'operation_number': 1364554}\n",
      "average env summary (episode 19): {'order_requirements': 1000000.0, 'container_shortage': 322683.0, 'operation_number': 1364554.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 20): {'order_requirements': 1000000, 'container_shortage': 349939, 'operation_number': 1253233}\n",
      "average env summary (episode 20): {'order_requirements': 1000000.0, 'container_shortage': 349939.0, 'operation_number': 1253233.0}\n",
      "\n",
      "Evaluation result:\n",
      "env summary (episode 20): {'order_requirements': 1000000, 'container_shortage': 762441, 'operation_number': 356108}\n",
      "average env summary (episode 20): {'order_requirements': 1000000.0, 'container_shortage': 762441.0, 'operation_number': 356108.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 21): {'order_requirements': 1000000, 'container_shortage': 498327, 'operation_number': 999438}\n",
      "average env summary (episode 21): {'order_requirements': 1000000.0, 'container_shortage': 498327.0, 'operation_number': 999438.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 22): {'order_requirements': 1000000, 'container_shortage': 359022, 'operation_number': 1206276}\n",
      "average env summary (episode 22): {'order_requirements': 1000000.0, 'container_shortage': 359022.0, 'operation_number': 1206276.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 23): {'order_requirements': 1000000, 'container_shortage': 346681, 'operation_number': 1326224}\n",
      "average env summary (episode 23): {'order_requirements': 1000000.0, 'container_shortage': 346681.0, 'operation_number': 1326224.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 24): {'order_requirements': 1000000, 'container_shortage': 233772, 'operation_number': 1477950}\n",
      "average env summary (episode 24): {'order_requirements': 1000000.0, 'container_shortage': 233772.0, 'operation_number': 1477950.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 25): {'order_requirements': 1000000, 'container_shortage': 330131, 'operation_number': 1266746}\n",
      "average env summary (episode 25): {'order_requirements': 1000000.0, 'container_shortage': 330131.0, 'operation_number': 1266746.0}\n",
      "\n",
      "Evaluation result:\n",
      "env summary (episode 25): {'order_requirements': 1000000, 'container_shortage': 761588, 'operation_number': 364701}\n",
      "average env summary (episode 25): {'order_requirements': 1000000.0, 'container_shortage': 761588.0, 'operation_number': 364701.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 26): {'order_requirements': 1000000, 'container_shortage': 492505, 'operation_number': 1007264}\n",
      "average env summary (episode 26): {'order_requirements': 1000000.0, 'container_shortage': 492505.0, 'operation_number': 1007264.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 27): {'order_requirements': 1000000, 'container_shortage': 372825, 'operation_number': 1229833}\n",
      "average env summary (episode 27): {'order_requirements': 1000000.0, 'container_shortage': 372825.0, 'operation_number': 1229833.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 28): {'order_requirements': 1000000, 'container_shortage': 351116, 'operation_number': 1311144}\n",
      "average env summary (episode 28): {'order_requirements': 1000000.0, 'container_shortage': 351116.0, 'operation_number': 1311144.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 29): {'order_requirements': 1000000, 'container_shortage': 381643, 'operation_number': 1169221}\n",
      "average env summary (episode 29): {'order_requirements': 1000000.0, 'container_shortage': 381643.0, 'operation_number': 1169221.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 30): {'order_requirements': 1000000, 'container_shortage': 372998, 'operation_number': 1209015}\n",
      "average env summary (episode 30): {'order_requirements': 1000000.0, 'container_shortage': 372998.0, 'operation_number': 1209015.0}\n",
      "\n",
      "Evaluation result:\n",
      "env summary (episode 30): {'order_requirements': 1000000, 'container_shortage': 757383, 'operation_number': 369697}\n",
      "average env summary (episode 30): {'order_requirements': 1000000.0, 'container_shortage': 757383.0, 'operation_number': 369697.0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env_sampler = rl_component_bundle.env_sampler\n",
    "\n",
    "num_episodes = 30\n",
    "eval_schedule = [5,10,15,20,25,30]\n",
    "eval_point_index = 0\n",
    "\n",
    "training_manager = TrainingManager(rl_component_bundle=rl_component_bundle)\n",
    "\n",
    "# main loop\n",
    "for ep in range(1, num_episodes + 1):\n",
    "    result = env_sampler.sample()\n",
    "    experiences: List[List[ExpElement]] = result[\"experiences\"]\n",
    "    info_list: List[dict] = result[\"info\"]\n",
    "        \n",
    "    print(\"Collecting result:\")\n",
    "    env_sampler.post_collect(info_list, ep)\n",
    "    print()\n",
    "\n",
    "    training_manager.record_experiences(experiences)\n",
    "    training_manager.train_step()\n",
    "\n",
    "    if ep == eval_schedule[eval_point_index]:\n",
    "        eval_point_index += 1\n",
    "        result = env_sampler.eval()\n",
    "        \n",
    "        print(\"Evaluation result:\")\n",
    "        env_sampler.post_evaluate(result[\"info\"], ep)\n",
    "        print()\n",
    "\n",
    "training_manager.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27bff38",
   "metadata": {},
   "source": [
    "# PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "162a8cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_net_conf = {\n",
    "    \"hidden_dims\": [256, 128, 64],\n",
    "    \"activation\": torch.nn.Tanh,\n",
    "    \"softmax\": True,\n",
    "    \"batch_norm\": False,\n",
    "    \"head\": True,\n",
    "}\n",
    "critic_net_conf = {\n",
    "    \"hidden_dims\": [256, 128, 64],\n",
    "    \"output_dim\": 1,\n",
    "    \"activation\": torch.nn.LeakyReLU,\n",
    "    \"softmax\": False,\n",
    "    \"batch_norm\": True,\n",
    "    \"head\": True,\n",
    "}\n",
    "\n",
    "actor_learning_rate = 0.001\n",
    "critic_learning_rate = 0.001\n",
    "\n",
    "class MyActorNet(DiscreteACBasedNet):\n",
    "    def __init__(self, state_dim: int, action_num: int) -> None:\n",
    "        super(MyActorNet, self).__init__(state_dim=state_dim, action_num=action_num)\n",
    "        self._actor = FullyConnected(input_dim=state_dim, output_dim=action_num, **actor_net_conf)\n",
    "        self._optim = Adam(self._actor.parameters(), lr=actor_learning_rate)\n",
    "\n",
    "    def _get_action_probs_impl(self, states: torch.Tensor) -> torch.Tensor:\n",
    "        return self._actor(states)\n",
    "\n",
    "\n",
    "class MyCriticNet(VNet):\n",
    "    def __init__(self, state_dim: int) -> None:\n",
    "        super(MyCriticNet, self).__init__(state_dim=state_dim)\n",
    "        self._critic = FullyConnected(input_dim=state_dim, **critic_net_conf)\n",
    "        self._optim = RMSprop(self._critic.parameters(), lr=critic_learning_rate)\n",
    "\n",
    "    def _get_v_values(self, states: torch.Tensor) -> torch.Tensor:\n",
    "        return self._critic(states).squeeze(-1)\n",
    "    \n",
    "def get_ppo_policy(state_dim: int, action_num: int, name: str) -> DiscretePolicyGradient:\n",
    "    return DiscretePolicyGradient(name=name, policy_net=MyActorNet(state_dim, action_num))\n",
    "\n",
    "\n",
    "def get_ppo(state_dim: int, name: str) -> PPOTrainer:\n",
    "    return PPOTrainer(\n",
    "        name=name,\n",
    "        reward_discount=0.0,\n",
    "        params=PPOParams(\n",
    "            get_v_critic_net_func=lambda: MyCriticNet(state_dim),\n",
    "            grad_iters=10,\n",
    "            critic_loss_cls=torch.nn.SmoothL1Loss,\n",
    "            lam=0.0,\n",
    "            clip_ratio=0.1,\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f5b4f93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_env = Env(scenario=\"cim\", topology=\"toy.4p_ssdd_l0.0\", durations=500,options={\"enable-dump-snapshot\": \"./ppo_learn\"})\n",
    "test_env = Env(scenario=\"cim\", topology=\"toy.4p_ssdd_l0.1\", durations=500,options={\"enable-dump-snapshot\": \"./ppo_test\"})\n",
    "\n",
    "num_agents = len(learn_env.agent_idx_list)\n",
    "agent2policy = {agent: f\"ppo_{agent}.policy\" for agent in learn_env.agent_idx_list}\n",
    "policies = [get_ppo_policy(state_dim, action_num, f\"ppo_{i}.policy\") for i in range(num_agents)]\n",
    "trainers = [get_ppo(state_dim, f\"ppo_{i}\") for i in range(num_agents)]\n",
    "\n",
    "rl_component_bundle = RLComponentBundle(\n",
    "    env_sampler=CIMEnvSampler(\n",
    "        learn_env=learn_env,\n",
    "        test_env=test_env,\n",
    "        policies=policies,\n",
    "        agent2policy=agent2policy,\n",
    "        reward_eval_delay=reward_shaping_conf[\"time_window\"],\n",
    "    ),\n",
    "    agent2policy=agent2policy,\n",
    "    policies=policies,\n",
    "    trainers=trainers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "50815f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting result:\n",
      "env summary (episode 1): {'order_requirements': 1000000, 'container_shortage': 673956, 'operation_number': 2061585}\n",
      "average env summary (episode 1): {'order_requirements': 1000000.0, 'container_shortage': 673956.0, 'operation_number': 2061585.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 2): {'order_requirements': 1000000, 'container_shortage': 754426, 'operation_number': 1576141}\n",
      "average env summary (episode 2): {'order_requirements': 1000000.0, 'container_shortage': 754426.0, 'operation_number': 1576141.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 3): {'order_requirements': 1000000, 'container_shortage': 600971, 'operation_number': 1919497}\n",
      "average env summary (episode 3): {'order_requirements': 1000000.0, 'container_shortage': 600971.0, 'operation_number': 1919497.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 4): {'order_requirements': 1000000, 'container_shortage': 580572, 'operation_number': 2018525}\n",
      "average env summary (episode 4): {'order_requirements': 1000000.0, 'container_shortage': 580572.0, 'operation_number': 2018525.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 5): {'order_requirements': 1000000, 'container_shortage': 581892, 'operation_number': 1919475}\n",
      "average env summary (episode 5): {'order_requirements': 1000000.0, 'container_shortage': 581892.0, 'operation_number': 1919475.0}\n",
      "\n",
      "Evaluation result:\n",
      "env summary (episode 5): {'order_requirements': 1000000, 'container_shortage': 952617, 'operation_number': 5628}\n",
      "average env summary (episode 5): {'order_requirements': 1000000.0, 'container_shortage': 952617.0, 'operation_number': 5628.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 6): {'order_requirements': 1000000, 'container_shortage': 667666, 'operation_number': 1719380}\n",
      "average env summary (episode 6): {'order_requirements': 1000000.0, 'container_shortage': 667666.0, 'operation_number': 1719380.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 7): {'order_requirements': 1000000, 'container_shortage': 635984, 'operation_number': 1939702}\n",
      "average env summary (episode 7): {'order_requirements': 1000000.0, 'container_shortage': 635984.0, 'operation_number': 1939702.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 8): {'order_requirements': 1000000, 'container_shortage': 503425, 'operation_number': 2207357}\n",
      "average env summary (episode 8): {'order_requirements': 1000000.0, 'container_shortage': 503425.0, 'operation_number': 2207357.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 9): {'order_requirements': 1000000, 'container_shortage': 460869, 'operation_number': 1966842}\n",
      "average env summary (episode 9): {'order_requirements': 1000000.0, 'container_shortage': 460869.0, 'operation_number': 1966842.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 10): {'order_requirements': 1000000, 'container_shortage': 404480, 'operation_number': 1746205}\n",
      "average env summary (episode 10): {'order_requirements': 1000000.0, 'container_shortage': 404480.0, 'operation_number': 1746205.0}\n",
      "\n",
      "Evaluation result:\n",
      "env summary (episode 10): {'order_requirements': 1000000, 'container_shortage': 950000, 'operation_number': 0}\n",
      "average env summary (episode 10): {'order_requirements': 1000000.0, 'container_shortage': 950000.0, 'operation_number': 0.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 11): {'order_requirements': 1000000, 'container_shortage': 512254, 'operation_number': 1917469}\n",
      "average env summary (episode 11): {'order_requirements': 1000000.0, 'container_shortage': 512254.0, 'operation_number': 1917469.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 12): {'order_requirements': 1000000, 'container_shortage': 444777, 'operation_number': 1997137}\n",
      "average env summary (episode 12): {'order_requirements': 1000000.0, 'container_shortage': 444777.0, 'operation_number': 1997137.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 13): {'order_requirements': 1000000, 'container_shortage': 456375, 'operation_number': 2066024}\n",
      "average env summary (episode 13): {'order_requirements': 1000000.0, 'container_shortage': 456375.0, 'operation_number': 2066024.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 14): {'order_requirements': 1000000, 'container_shortage': 438935, 'operation_number': 1904712}\n",
      "average env summary (episode 14): {'order_requirements': 1000000.0, 'container_shortage': 438935.0, 'operation_number': 1904712.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 15): {'order_requirements': 1000000, 'container_shortage': 298393, 'operation_number': 2002171}\n",
      "average env summary (episode 15): {'order_requirements': 1000000.0, 'container_shortage': 298393.0, 'operation_number': 2002171.0}\n",
      "\n",
      "Evaluation result:\n",
      "env summary (episode 15): {'order_requirements': 1000000, 'container_shortage': 950000, 'operation_number': 0}\n",
      "average env summary (episode 15): {'order_requirements': 1000000.0, 'container_shortage': 950000.0, 'operation_number': 0.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 16): {'order_requirements': 1000000, 'container_shortage': 343966, 'operation_number': 1812910}\n",
      "average env summary (episode 16): {'order_requirements': 1000000.0, 'container_shortage': 343966.0, 'operation_number': 1812910.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 17): {'order_requirements': 1000000, 'container_shortage': 294648, 'operation_number': 2020196}\n",
      "average env summary (episode 17): {'order_requirements': 1000000.0, 'container_shortage': 294648.0, 'operation_number': 2020196.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 18): {'order_requirements': 1000000, 'container_shortage': 361816, 'operation_number': 1804699}\n",
      "average env summary (episode 18): {'order_requirements': 1000000.0, 'container_shortage': 361816.0, 'operation_number': 1804699.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 19): {'order_requirements': 1000000, 'container_shortage': 373420, 'operation_number': 1921588}\n",
      "average env summary (episode 19): {'order_requirements': 1000000.0, 'container_shortage': 373420.0, 'operation_number': 1921588.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 20): {'order_requirements': 1000000, 'container_shortage': 278674, 'operation_number': 2051903}\n",
      "average env summary (episode 20): {'order_requirements': 1000000.0, 'container_shortage': 278674.0, 'operation_number': 2051903.0}\n",
      "\n",
      "Evaluation result:\n",
      "env summary (episode 20): {'order_requirements': 1000000, 'container_shortage': 538832, 'operation_number': 793380}\n",
      "average env summary (episode 20): {'order_requirements': 1000000.0, 'container_shortage': 538832.0, 'operation_number': 793380.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 21): {'order_requirements': 1000000, 'container_shortage': 262799, 'operation_number': 1967083}\n",
      "average env summary (episode 21): {'order_requirements': 1000000.0, 'container_shortage': 262799.0, 'operation_number': 1967083.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 22): {'order_requirements': 1000000, 'container_shortage': 274495, 'operation_number': 1965629}\n",
      "average env summary (episode 22): {'order_requirements': 1000000.0, 'container_shortage': 274495.0, 'operation_number': 1965629.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 23): {'order_requirements': 1000000, 'container_shortage': 247907, 'operation_number': 1837970}\n",
      "average env summary (episode 23): {'order_requirements': 1000000.0, 'container_shortage': 247907.0, 'operation_number': 1837970.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 24): {'order_requirements': 1000000, 'container_shortage': 292586, 'operation_number': 2032765}\n",
      "average env summary (episode 24): {'order_requirements': 1000000.0, 'container_shortage': 292586.0, 'operation_number': 2032765.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 25): {'order_requirements': 1000000, 'container_shortage': 172540, 'operation_number': 1906035}\n",
      "average env summary (episode 25): {'order_requirements': 1000000.0, 'container_shortage': 172540.0, 'operation_number': 1906035.0}\n",
      "\n",
      "Evaluation result:\n",
      "env summary (episode 25): {'order_requirements': 1000000, 'container_shortage': 490828, 'operation_number': 884353}\n",
      "average env summary (episode 25): {'order_requirements': 1000000.0, 'container_shortage': 490828.0, 'operation_number': 884353.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 26): {'order_requirements': 1000000, 'container_shortage': 116412, 'operation_number': 1915086}\n",
      "average env summary (episode 26): {'order_requirements': 1000000.0, 'container_shortage': 116412.0, 'operation_number': 1915086.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 27): {'order_requirements': 1000000, 'container_shortage': 313009, 'operation_number': 1642655}\n",
      "average env summary (episode 27): {'order_requirements': 1000000.0, 'container_shortage': 313009.0, 'operation_number': 1642655.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 28): {'order_requirements': 1000000, 'container_shortage': 233339, 'operation_number': 1879649}\n",
      "average env summary (episode 28): {'order_requirements': 1000000.0, 'container_shortage': 233339.0, 'operation_number': 1879649.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 29): {'order_requirements': 1000000, 'container_shortage': 212434, 'operation_number': 1910722}\n",
      "average env summary (episode 29): {'order_requirements': 1000000.0, 'container_shortage': 212434.0, 'operation_number': 1910722.0}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 30): {'order_requirements': 1000000, 'container_shortage': 219383, 'operation_number': 1732693}\n",
      "average env summary (episode 30): {'order_requirements': 1000000.0, 'container_shortage': 219383.0, 'operation_number': 1732693.0}\n",
      "\n",
      "Evaluation result:\n",
      "env summary (episode 30): {'order_requirements': 1000000, 'container_shortage': 536230, 'operation_number': 795302}\n",
      "average env summary (episode 30): {'order_requirements': 1000000.0, 'container_shortage': 536230.0, 'operation_number': 795302.0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env_sampler = rl_component_bundle.env_sampler\n",
    "\n",
    "num_episodes = 30\n",
    "eval_schedule = [5,10,15,20,25,30]\n",
    "eval_point_index = 0\n",
    "\n",
    "training_manager = TrainingManager(rl_component_bundle=rl_component_bundle)\n",
    "\n",
    "# main loop\n",
    "for ep in range(1, num_episodes + 1):\n",
    "    result = env_sampler.sample()\n",
    "    experiences: List[List[ExpElement]] = result[\"experiences\"]\n",
    "    info_list: List[dict] = result[\"info\"]\n",
    "        \n",
    "    print(\"Collecting result:\")\n",
    "    env_sampler.post_collect(info_list, ep)\n",
    "    print()\n",
    "\n",
    "    training_manager.record_experiences(experiences)\n",
    "    training_manager.train_step()\n",
    "\n",
    "    if ep == eval_schedule[eval_point_index]:\n",
    "        eval_point_index += 1\n",
    "        result = env_sampler.eval()\n",
    "        \n",
    "        print(\"Evaluation result:\")\n",
    "        env_sampler.post_evaluate(result[\"info\"], ep)\n",
    "        print()\n",
    "\n",
    "training_manager.exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
